{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d2ee6f",
   "metadata": {},
   "source": [
    "# Llama-cpp\n",
    "\n",
    "이 노트북은 LangChain 내에서 Llama-cpp 임베딩을 사용하는 방법에 대해 설명합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601905ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da6f70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH08-Embeddings\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH08-Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6482d4",
   "metadata": {},
   "source": [
    "llama-cpp-python 패키지를 최신 버전으로 업그레이드하는 pip 명령어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bc0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "# !pip install -qU llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08367d",
   "metadata": {},
   "source": [
    "- `LlamaCppEmbeddings` 클래스를 `langchain_community.embeddings` 모듈에서 임포트합니다.\n",
    "\n",
    "`LlamaCppEmbeddings`는 LLaMA 모델을 사용하여 텍스트 임베딩을 생성하는 클래스입니다. \n",
    "\n",
    "이 클래스는 LangChain 커뮤니티에서 제공하는 확장 기능 중 하나로, C++로 구현된 LLaMA 모델을 활용하여 빠르고 효율적인 임베딩 생성을 지원합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f274cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import LlamaCppEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48f9f4",
   "metadata": {},
   "source": [
    "- `LlamaCppEmbeddings` 클래스를 사용하여 임베딩 모델을 초기화합니다.\n",
    "- `model_path` 매개변수를 통해 사전 학습된 LLaMA 모델 파일(\"ggml-model-q4_0.bin\")의 경로를 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e814921",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/teddy/lib/python3.11/site-packages/langchain_community/embeddings/llamacpp.py:92\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m Llama(model_path, embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LlamaCpp 임베딩 모델을 초기화하고, 모델 경로를 지정합니다.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llama \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCppEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/path/to/model/ggml-model-q4_0.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/teddy/lib/python3.11/site-packages/langchain_community/embeddings/llamacpp.py:96\u001b[0m, in \u001b[0;36mLlamaCppEmbeddings.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m Llama(model_path, embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_params)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import llama-cpp-python library. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install the llama-cpp-python library to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse this embedding model: pip install llama-cpp-python\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load Llama model from path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m     )\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import llama-cpp-python library. Please install the llama-cpp-python library to use this embedding model: pip install llama-cpp-python"
     ]
    }
   ],
   "source": [
    "# LlamaCpp 임베딩 모델을 초기화하고, 모델 경로를 지정합니다.\n",
    "llama = LlamaCppEmbeddings(model_path=\"/path/to/model/ggml-model-q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3b136",
   "metadata": {},
   "source": [
    "- `text` 변수에 \"This is a test document.\"라는 문자열을 할당합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53970f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\"  # 테스트용 문서 텍스트를 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a723236",
   "metadata": {},
   "source": [
    "`llama.embed_query(text)`는 입력된 텍스트를 임베딩 벡터로 변환하는 함수입니다.\n",
    "\n",
    "- `text` 매개변수로 전달된 텍스트를 임베딩 모델에 입력하여 벡터 표현을 생성합니다.\n",
    "- 생성된 임베딩 벡터는 `query_result` 변수에 저장됩니다.\n",
    "\n",
    "이 함수는 텍스트를 벡터 공간에 매핑하여 의미적 유사성을 계산하거나 검색에 활용할 수 있는 벡터 표현을 얻는 데 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 임베딩하여 쿼리 결과를 생성합니다.\n",
    "query_result = llama.embed_query(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9091dfb",
   "metadata": {},
   "source": [
    "`llama.embed_documents([text])` 함수를 호출하여 `text` 문서를 임베딩합니다.\n",
    "\n",
    "- `text` 문서를 리스트 형태로 `embed_documents` 함수에 전달합니다.\n",
    "- `llama` 객체의 `embed_documents` 함수는 문서를 벡터 표현으로 변환합니다.\n",
    "- 변환된 벡터 표현은 `doc_result` 변수에 저장됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ae3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 임베딩하여 문서 결과를 생성합니다.\n",
    "doc_result = llama.embed_documents([text])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
